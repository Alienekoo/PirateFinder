{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymongo\n",
    "#from sklearn import neural_network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import sys\n",
    "import math\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SMOTENC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "mydb = client[\"flowdb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train_coll = mydb[\"testP_agg\"]\n",
    "p_test_coll = mydb[\"trainP_agg\"]\n",
    "b_train_coll = mydb[\"trainB_agg\"]\n",
    "b_test_coll = mydb[\"testB_agg\"]\n",
    "\n",
    "p_train = []\n",
    "p_test = []\n",
    "b_train = []\n",
    "b_test = []\n",
    "p_train_l = []\n",
    "p_test_l = []\n",
    "b_train_l = []\n",
    "b_test_l = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMD(doc):\n",
    "    tmp = []\n",
    "    if doc['sp'][0] == None:\n",
    "        tmp.append(-1)\n",
    "    else:\n",
    "        tmp.append(doc['sp'][0])\n",
    "        \n",
    "    if doc['dp'][0] == None:\n",
    "        tmp.append(-1)\n",
    "    else:\n",
    "        tmp.append(doc['dp'][0])\n",
    "    #tmp.append(doc['sp'][0])\n",
    "    #tmp.append(doc['dp'][0])\n",
    "    if doc['Source ASN'] == None:\n",
    "        tmp.append(-1)\n",
    "    else:\n",
    "        tmp.append(doc['Source ASN'])\n",
    "    if doc['Dest ASN'] == None:\n",
    "        tmp.append(-1)\n",
    "    else:\n",
    "        tmp.append(doc['Dest ASN'])\n",
    "    tmp.append(doc['bytes_in'])\n",
    "    tmp.append(doc['bytes_out'])\n",
    "    tmp.append(doc['total_bytes'])\n",
    "    tmp.append(doc['num_pkts_in'])\n",
    "    tmp.append(doc['num_pkts_out'])\n",
    "    tmp.append(doc['total_num_pkts'])\n",
    "    tmp.append(doc['# of flows'])\n",
    "    for i in range(256):\n",
    "        if (doc['byte_dist'][i] != None):\n",
    "            try:\n",
    "                tmp.append(doc['byte_dist'][i]/doc['total_bytes'])\n",
    "            except:\n",
    "                tmp.append(0)\n",
    "        else:\n",
    "            tmp.append(0)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIFTBuckets(doc):\n",
    "    tmp = []\n",
    "    ifts = doc['ift']\n",
    "    length = len(ifts)\n",
    "    tmp.append(length)\n",
    "    #tmp.append(np.mean(ifts))\n",
    "    num = 15\n",
    "    buckets = [0] * num\n",
    "    sum = 0\n",
    "    avg_ift = 0\n",
    "    for ift in ifts:\n",
    "        if ift > 0:\n",
    "           logift = math.log(ift, 2)\n",
    "        if ift == 0:\n",
    "            buckets[0] += 1\n",
    "        elif logift <= 0:\n",
    "            buckets[0] += 1\n",
    "            sum += ift\n",
    "        elif logift >= 14:\n",
    "            buckets[14] += 1\n",
    "            sum += ift\n",
    "        else:\n",
    "            buckets[int(math.floor(logift))] += 1\n",
    "            sum += ift\n",
    "    \n",
    "    if length > 1:\n",
    "        avg_ift = sum / length\n",
    "    tmp.append(avg_ift)\n",
    "    tmp.extend(buckets)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndividualFL(doc):\n",
    "    numRows = 25\n",
    "    binSize = 200\n",
    "    transMat = np.zeros((numRows, numRows))\n",
    "    if len(doc['byte_array']) == 0:\n",
    "        return list(transMat.flatten())\n",
    "    elif len(doc['byte_array']) == 1:\n",
    "        curFlowSize = min(int(doc['byte_array'][0] / binSize), numRows - 1)\n",
    "        transMat[curFlowSize, curFlowSize] = 1\n",
    "        return list(transMat.flatten())\n",
    "\n",
    "    for i in range(1, len(doc['byte_array'])):\n",
    "        prevFlowSize = min(int(doc['byte_array'][i-1] / binSize), numRows - 1)\n",
    "        curFlowSize = min(int(doc['byte_array'][i] / binSize), numRows - 1)\n",
    "        transMat[prevFlowSize, curFlowSize] += 1\n",
    "\n",
    "    for i in range(numRows):\n",
    "        if float(np.sum(transMat[i:i+1])) != 0:\n",
    "            transMat[i:i+1] = transMat[i:i+1] / float(np.sum(transMat[i:i + 1]))\n",
    "    return list(transMat.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up training and testing data arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in p_train_coll.find():\n",
    "    tmpd = []\n",
    "    tmp = getMD(doc)\n",
    "    tmpift = getIFTBuckets(doc)\n",
    "    #tmpind_fl = getIndividualFL(doc)\n",
    "    \n",
    "    \n",
    "    tmpd.extend(tmp)\n",
    "    tmpd.extend(tmpift)\n",
    "    #tmpd.extend(tmpind_fl)\n",
    "    p_train.append(tmpd)\n",
    "    p_train_l.append(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for doc in p_test_coll.find():\n",
    "    tmpd = []\n",
    "    tmp = getMD(doc)\n",
    "    tmpift = getIFTBuckets(doc)\n",
    "    #tmpind_fl = getIndividualFL(doc)\n",
    "    \n",
    "    \n",
    "    tmpd.extend(tmp)\n",
    "    tmpd.extend(tmpift)\n",
    "    #tmpd.extend(tmpind_fl)\n",
    "    p_test.append(tmpd)\n",
    "    p_test_l.append(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in b_train_coll.find():\n",
    "    tmpd = []\n",
    "    tmp = getMD(doc)\n",
    "    tmpift = getIFTBuckets(doc)\n",
    "    #tmpind_fl = getIndividualFL(doc)\n",
    "    \n",
    "    \n",
    "    tmpd.extend(tmp)\n",
    "    tmpd.extend(tmpift)\n",
    "    #tmpd.extend(tmpind_fl)\n",
    "    b_train.append(tmpd)\n",
    "    b_train_l.append(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in b_test_coll.find():\n",
    "    tmpd = []\n",
    "    tmp = getMD(doc)\n",
    "    tmpift = getIFTBuckets(doc)\n",
    "    #tmpind_fl = getIndividualFL(doc)\n",
    "    \n",
    "    \n",
    "    tmpd.extend(tmp)\n",
    "    tmpd.extend(tmpift)\n",
    "    #tmpd.extend(tmpind_fl)\n",
    "    b_test.append(tmpd)\n",
    "    b_test_l.append(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = []\n",
    "training_set.extend(b_train)\n",
    "training_set.extend(p_train)\n",
    "training_labels = []\n",
    "training_labels.extend(b_train_l)\n",
    "training_labels.extend(p_train_l)\n",
    "training_set = np.array(training_set)\n",
    "training_labels = np.array(training_labels)\n",
    "b_train = np.array(b_train)\n",
    "p_train = np.array(p_train)\n",
    "b_train_l = np.array(b_train_l)\n",
    "p_train_l = np.array(p_train_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_set = []\n",
    "test_labels = []\n",
    "test_set.extend(b_test)\n",
    "test_set.extend(p_test)\n",
    "test_labels.extend(b_test_l)\n",
    "test_labels.extend(p_test_l)\n",
    "test_set = np.array(test_set)\n",
    "test_labels = np.array(test_labels)\n",
    "b_test = np.array(b_test)\n",
    "p_test = np.array(p_test)\n",
    "b_test_l = np.array(b_test_l)\n",
    "p_test_l = np.array(p_test_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = MLPClassifier(learning_rate_init=0.01)\n",
    "decision_tree = DecisionTreeClassifier(criterion='entropy')\n",
    "#params = {'learning_rate_init': [0.001, 0.01, 0.1, 1]}\n",
    "#nn = GridSearchCV(MLPClassifier(), params, scoring=make_scorer(accuracy_score),cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((2369, 284), (2369,))\n",
      "((4318, 284), (4318,))\n"
     ]
    }
   ],
   "source": [
    "#nn.fit(training_set, training_labels)\n",
    "#print nn.cv_results_\n",
    "print (training_set.shape, training_labels.shape)\n",
    "smote = SMOTE(1.0)\n",
    "#smotenc = SMOTENC(1.0)\n",
    "rs_x, rs_y = smote.fit_sample(training_set, training_labels)\n",
    "#nc_x, nc_y = smotenc.fit_sample(training_set, training_labels)\n",
    "print(rs_x.shape, rs_y.shape)\n",
    "#print(nc_x.shape, nc_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size, train_scores, test_scores = learning_curve(neural_net, training_set, training_labels, cv=5, train_sizes=np.linspace(0.1,1,10), \n",
    "                                                       scoring=make_scorer(accuracy_score), n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 189  379  568  758  947 1137 1326 1516 1705 1895]\n",
      "0.9889920844327176\n",
      "0.9062068134985415\n"
     ]
    }
   ],
   "source": [
    "print (train_size)\n",
    "print (np.mean(train_scores))\n",
    "print (np.mean(test_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "       learning_rate_init=0.01, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_net.fit(training_set, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6905537459283387\n",
      "0.9904761904761905\n",
      "0.041237113402061855\n"
     ]
    }
   ],
   "source": [
    "print('scores on full test set, just benign, and just pirate, without increasing piracy sample set')\n",
    "print(neural_net.score(test_set, test_labels))\n",
    "print(neural_net.score(b_test, b_test_l))\n",
    "print(neural_net.score(p_test, p_test_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 189  379  568  758  947 1137 1326 1516 1705 1895]\n",
      "0.9999683377308708\n",
      "0.9075919929349425\n",
      "[ 189  379  568  758  947 1137 1326 1516 1705 1895]\n",
      "0.9999683377308708\n",
      "0.9082695069624713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size, train_scores, test_scores = learning_curve(decision_tree, training_set, training_labels, cv=5, train_sizes=np.linspace(0.1,1,10), \n",
    "                                                       scoring=make_scorer(accuracy_score), n_jobs=-1)\n",
    "print (train_size)\n",
    "print (np.mean(train_scores))\n",
    "print (np.mean(test_scores))\n",
    "train_size, train_scores, test_scores = learning_curve(decision_tree, training_set, training_labels, cv=5, train_sizes=np.linspace(0.1,1,10), \n",
    "                                                       scoring=make_scorer(accuracy_score), n_jobs=-1)\n",
    "print (train_size)\n",
    "print (np.mean(train_scores))\n",
    "print (np.mean(test_scores))\n",
    "decision_tree.fit(training_set, training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6384364820846905\n",
      "0.6904761904761905\n",
      "0.5257731958762887\n"
     ]
    }
   ],
   "source": [
    "print('scores on full test set, just benign, and just pirate, without increasing piracy sample set')\n",
    "print(decision_tree.score(test_set, test_labels))\n",
    "print(decision_tree.score(b_test, b_test_l))\n",
    "print(decision_tree.score(p_test, p_test_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6710097719869706\n",
      "0.7571428571428571\n",
      "0.4845360824742268\n",
      "0.6319218241042345\n",
      "0.6666666666666666\n",
      "0.5567010309278351\n"
     ]
    }
   ],
   "source": [
    "neural_net.fit(rs_x, rs_y)\n",
    "decision_tree.fit(rs_x, rs_y)\n",
    "\n",
    "print('scores on full test set, just benign, and just pirate')\n",
    "print(neural_net.score(test_set, test_labels))\n",
    "print(neural_net.score(b_test, b_test_l))\n",
    "print(neural_net.score(p_test, p_test_l))\n",
    "print('scores on full test set, just benign, and just pirate')\n",
    "print(decision_tree.score(test_set, test_labels))\n",
    "print(decision_tree.score(b_test, b_test_l))\n",
    "print(decision_tree.score(p_test, p_test_l))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
